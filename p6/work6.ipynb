{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实验要求\n",
    "### 截止日期：12月15日\n",
    "作业的提交格式参考之前的说明，提交到18329300691@163.com\n",
    "### 基本要求\n",
    "a)\t基于 Watermelon-train1数据集（只有离散属性），构造ID3决策树；\n",
    "b)\t基于构造的 ID3 决策树，对数据集 Watermelon-test1进行预测，输出分类精度；\n",
    "### 中级要求\n",
    "a)  对数据集Watermelon-train2，构造C4.5或者CART决策树，要求可以处理连续型属性；\n",
    "b)\t对测试集Watermelon-test2进行预测，输出分类精度；\n",
    "### 高级要求\n",
    "使用任意的剪枝算法对构造的决策树（基本要求和中级要求构造的树）进行剪枝，观察测试集合的分类精度是否有提升，给出分析过程。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的划分\n",
    "- 决策树主要分为三种：\n",
    "\tID3，C4.5和CART，它们分别对应的**特征选择准则**是信息增益（ID3），信息增益比（C4.5）和基尼指数（CART）。\n",
    "\t它们决定当前选择哪个特征进行数据划分，使得样本在当下能够被最大程度的划分。\n",
    "- 对于离散变量，选定**属性**分类即可；\n",
    "- 对于连续变量，需要选定**划分点**。\n",
    "- CART和C4.5支持数据特征为**连续分布**时的处理，能够完成对连续属性的离散化处理，主要通过二元切分的方式来处理连续型变量，这个分裂点的选择原则是使得划分后的子树中的“混乱程度”降低。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858376ed-24be-490a-9891-38433df1bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import copy\n",
    "def read(name):\n",
    "    df = pd.read_csv(name, error_bad_lines = False, encoding = 'gbk')\n",
    "    temp = np.array(df).tolist()\n",
    "    for i in temp:\n",
    "        i.pop(0)\n",
    "    return temp\n",
    "\n",
    "train1 = read(\"D:\\dasanshang\\jiqixuexi\\p6\\Watermelon-train1.csv\")\n",
    "train2 = read(\"D:\\dasanshang\\jiqixuexi\\p6\\Watermelon-train2.csv\")\n",
    "test1 = read(\"D:\\dasanshang\\jiqixuexi\\p6\\Watermelon-test1.csv\")\n",
    "test2 = read(\"D:\\dasanshang\\jiqixuexi\\p6\\Watermelon-test2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先进行数据的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832dc417-b47d-45df-b03b-7621aaf8b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inport(data):\n",
    "    dic = {}\n",
    "    for i in data:\n",
    "        current = i[-1] #取出最后的结果\n",
    "        if current not in dic.keys(): \n",
    "            dic[current] = 1 #创建一个新的类别\n",
    "        else:\n",
    "            dic[current] += 1 #原有类别+1\n",
    "    result = 0.0\n",
    "    for key in dic:\n",
    "        pro = float(dic[key]) / len(data)\n",
    "        result -= pro * math.log(pro,2) \n",
    "    return result, dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算信息增益\n",
    "- 统计某结果数据发生的频率，每项的信息以字典的形似存储\n",
    "- 计算信息熵\n",
    "- 返回信息熵和分类信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd7ae58-4319-4aef-9539-424a4629d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen(data, index, kind):\n",
    "    ls = []\n",
    "    for temp in data:\n",
    "        if temp[index] == kind:\n",
    "            t = temp[0: index]\n",
    "            t = t + temp[index + 1: ]\n",
    "            ls.append(t)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将数据按照某一种类的属性重新分类，并将该行属性删除\n",
    "- 计算信息熵和信息增益并返回最优的分类方式，对数据的每项指标做以下的过程\n",
    "    1. 抽取该项数据的所有信息\n",
    "    2. 按照该项数据的类别信息将数据集划分成多个子数据集\n",
    "    3. 计算每个数据集的信息熵\n",
    "    5. 计算该项数据的信息增益\n",
    "    6. 根据信息增益选择最好的分类项目，返回该项目的类别号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7258bb-3dd0-4e37-a581-ae12794e8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zuijiatezhengfenli(data):\n",
    "    base, mm= inport(data) #原始的信息熵\n",
    "    best = 0\n",
    "    bestindex = -1\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        #抽取该列数据的所有信息\n",
    "        ls = [index[i] for index in data]\n",
    "        feture = set(ls) \n",
    "        #计算该列的信息增益\n",
    "        temp = 0.0\n",
    "        for value in feture:\n",
    "            datatemp = split(data, i, value)\n",
    "            prob = len(datatemp) / float(len(data))\n",
    "            t, mm = information(datatemp)\n",
    "            temp += prob * t\n",
    "        infoGain = base - temp\n",
    "        #根据信息增益挑选 \n",
    "        if infoGain > best:\n",
    "            best = infoGain\n",
    "            bestindex = i\n",
    "    return bestindex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3算法\n",
    "- ID3算法的核⼼思想应用信息增益准则作为标准,介绍信息增益之前首先介绍一下信息熵和条件熵： \n",
    "- 熵（entropy）概念：\n",
    "\t    1948年，香农提出了“信息熵”的概念。在信息论与概率统计中，熵是表示随机变量不确定性的量。X是⼀个取值为有限个的离散随机变量，\n",
    "$$ H(X)=-\\sum_{i=1}^{n} p\\left(x_{i}\\right) \\log p\\left(x_{i}\\right)$$ \n",
    "$𝐻(𝑋)$就被称作随机变量𝑋的熵，它表示随机变量不确定的度量。熵取值越大，随机变量不确定性越大。当随机变量为均匀分布时，熵最大。当某一状态概率取值为1时，熵的值为零。\n",
    "\n",
    "### ID3算法-条件熵和信息增益\n",
    "- 条件熵 $𝐻(𝑌∣𝑋)$ ：\n",
    "\t表示在已知随机变量𝑋的条件下随机变量𝑌的不确定性，定义为给定𝑋条件下𝑌的条件概率分布的熵对𝑋的数学期望:\n",
    "$$H(Y \\mid X)=\\sum_{x} p(x) H(Y \\mid X=x) =-\\sum_{x} p(x) \\sum_{y} p(y \\mid x) \\log p(y \\mid x)$$\n",
    "\n",
    "- 特征𝐴对数据集𝐷的信息增益就是熵$𝐻(𝐷)$与条件熵$𝐻(𝐷|𝐴)$之差:\n",
    "$$𝐻(𝐷)−𝐻(𝐷∣𝐴)$$\n",
    "\n",
    "\t表示已知特征𝐴的信息而使得数据集𝐷的信息不确定减少的程度。信息增益越大的特征代表其具有更强的分类能力，所以我们就要**选择能够使数据的不确定程度减少最多的特征**，也就是信息增益最大的特征。\n",
    "\n",
    "### ID3算法-停止条件\n",
    "- 决策树的生成:\n",
    "\n",
    "\t从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "    \n",
    "    \n",
    "- 迭代停止条件：\n",
    "  1. 当前结点所有样本都属于同⼀类别；\n",
    "  2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "  3. 达到最大树深；\n",
    "  4. 达到叶子结点的最小样本数；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "182687ce-b400-412e-bee8-9b8930b7a009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen1(data, labels):\n",
    "    typelist = [index[-1] for index in data] #取出该数据集的分类\n",
    "    nothing, typecount = information(data) #计算出类别种类以及数量\n",
    "    if len(typecount) == 1: #如果只有一个类别\n",
    "        return typelist[0]\n",
    "    bestindex = zuijiatezhengfenli(data)  # 最优划分属性的索引\n",
    "    bestlabel = labels[bestindex]\n",
    "    Tree = {bestlabel: {}}\n",
    "    temp = labels[:]\n",
    "    del (temp[bestindex])  # 已经选择的特征不再参与分类，将该类别删除\n",
    "    feature = [example[bestindex] for example in data]\n",
    "    unique = set(feature)  # 该属性所有可能取值，也就是节点的分支\n",
    "    for i in unique:  \n",
    "        temp = temp[:] \n",
    "        Tree[bestlabel][i] = classify1(split(data, bestindex, i), temp)\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归构建$ID3$决策树\n",
    "- 决策树的生成\n",
    "\t1. 从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t2. 在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "- 迭代停止条件\n",
    "    1. 当前结点所有样本都属于同⼀类别；\n",
    "    2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "    3. 达到最大树深；\n",
    "    4. 达到叶子结点的最小样本数；\n",
    "- 具体实现\n",
    "    1. 首先取出该数据集的类别信息\n",
    "    2. 统计处类别信息以及数量，如果只有一个类别，返回该类别\n",
    "    3. 计算最优划分的索引\n",
    "    4. 初始化子树\n",
    "    5. 当前已经选择的特征不再参与分类，将该特征删除\n",
    "    6. 计算剩余特征的集合\n",
    "    7. 对于每个分支，进行递归\n",
    "    8. 返回值为子树\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "655536c5-969d-40de-8286-9da9eac6d962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce1(testdata, tree, labels):\n",
    "    firstStr = list(tree.keys())[0]\n",
    "    secondDict = tree[firstStr]\n",
    "    featIndex = labels.index(firstStr)\n",
    "    result = ''\n",
    "    for key in list(secondDict.keys()): \n",
    "         if testdata[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                result = run1(testdata, secondDict[key], labels)\n",
    "            else:\n",
    "                result = secondDict[key]\n",
    "    return result\n",
    "\n",
    "def shengc(train, test, labels):\n",
    "    ls = []\n",
    "    tree = fen2(train, labels)\n",
    "    print(\"生成决策树如下:\", tree)\n",
    "    for index in test:\n",
    "        ls.append(run1(index, tree, labels))\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集利用$ID3$决策树进行分类\n",
    "- 取出当前树的根节点\n",
    "- 利用跟节点信息查询当前输入数据输入内容\n",
    "- 如果查询出来的分支是叶节点，返回该值\n",
    "- 如果不是叶节点，递归查询子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d71afdc1-ac32-435b-b50a-fd33f32bd25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成决策树如下: {'纹理': {'清晰': {'根蒂': {'稍蜷': '是', '蜷缩': '是', '硬挺': '否'}}, '模糊': '否', '稍糊': {'色泽': {'青绿': '否', '浅白': '否', '乌黑': {'敲声': {'浊响': '是', '沉闷': '否'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "labels1 = ['色泽', '根蒂', '敲声', '纹理', '好瓜']\n",
    "result1 = shengc(train1, test1, labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的剪枝\n",
    "- 决策树很容易出现**过拟合现象**。原因在于学习时完全考虑的是如何提⾼对训练数据的正确分类从⽽构建出过于复杂的决策树。\n",
    "- 解决这个问题的方法称为**剪枝**，即对已生成的树进行简化。具体地，就是从已生成的树上裁剪掉⼀些子树或叶节点，并将其根节点或父节点作为新的叶节点。 \n",
    "- 决策树的剪枝基本策略有**预剪枝 (Pre-Pruning)** 和 **后剪枝 (Post-Pruning)**\n",
    "   - **预剪枝**：是根据⼀些原则**极早的停止树增长**，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的幅度小于用户指定的幅度等。 \n",
    "   - **后剪枝**：是通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点。是在生成决策树之后**自底向上**的对树中所有的非叶结点进⾏逐一考察 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4397f502-09e7-4b49-89d1-46370c4ba337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成决策树如下: {'纹理': {'清晰': {'根蒂': {'稍蜷': '是', '蜷缩': '是', '硬挺': '否'}}, '模糊': '否', '稍糊': {'色泽': {'青绿': '否', '浅白': '否', '乌黑': {'敲声': {'浊响': '是', '沉闷': '否'}}}}}}\n",
      "剪枝后的决策树如下: {'纹理': {'清晰': {'根蒂': {'稍蜷': '是', '蜷缩': '是', '硬挺': '否'}}, '模糊': '否', '稍糊': {'色泽': {'青绿': '否', '浅白': '否', '乌黑': {'敲声': {'浊响': '是', '沉闷': '否'}}}}}}\n"
     ]
    }
   ],
   "source": [
    "def prune(tree, testdata, testlabels):\n",
    "    firstStr = list(tree.keys())[0]\n",
    "    secondDict = tree[firstStr]\n",
    "    \n",
    "    # 判断是否为叶子节点，如果是叶子节点，则进行剪枝操作\n",
    "    if type(secondDict).__name__ != 'dict':\n",
    "        return secondDict\n",
    "    \n",
    "    featIndex = testlabels.index(firstStr)\n",
    "    result = ''\n",
    "    for key in list(secondDict.keys()):\n",
    "        if testdata[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                result = prune(secondDict[key], testdata, testlabels)\n",
    "            else:\n",
    "                result = secondDict[key]\n",
    "    \n",
    "    # 基于剪枝后的子树进行后剪枝操作\n",
    "    if all(val == result for val in secondDict.values()):\n",
    "        return result\n",
    "    else:\n",
    "        return tree\n",
    "\n",
    "# 使用剪枝后的决策树进行分类\n",
    "def get_pruned_result(train, test, labels):\n",
    "    ls = []\n",
    "    tree = classify1(train, labels)\n",
    "    print(\"生成决策树如下:\", tree)\n",
    "    pruned_tree = prune(tree, test, labels)\n",
    "    print(\"剪枝后的决策树如下:\", pruned_tree)\n",
    "    for index in test:\n",
    "        ls.append(run1(index, pruned_tree, labels))\n",
    "    return ls\n",
    "\n",
    "# 调用剪枝函数对决策树进行剪枝并进行分类\n",
    "pruned_result1 = get_pruned_result(train1, test1, labels1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- accuracy函数用于计算分类准确率。post_pruning2函数实现了后剪枝操作，其中validation_data是用于验证剪枝效果的数据集。\n",
    "\n",
    "- 剪枝后的决策树pruned_tree可以用于进行分类预测，可以使用run2函数对新的数据进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf20f37e-52f0-4ea3-a6d4-1e903385a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID3分类器对于test1数据集的准确率是： 70.00%\n"
     ]
    }
   ],
   "source": [
    "def rate(data, predict):\n",
    "    num = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i][-1] == predict[i]:\n",
    "            num +=1\n",
    "    return format(num / len(data), '.2%')\n",
    "print(\"ID3分类器对于test1数据集的准确率是：\", rate(test1, result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率\n",
    "可以看出，用这种方法得到树分类树对于小规模数据的分类效果相对较好。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd4ea474-bddb-4df5-87e6-eac03d8efda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen2(data):\n",
    "    ls = data[:]\n",
    "    ls.sort()\n",
    "    result = []\n",
    "    for i in range(len(ls) - 1):\n",
    "        result.append((data[i + 1] + data[i]) / 2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C4.5算法\n",
    "- C4.5算法与ID3算法相似，其对ID3算法进行了改进。\n",
    "- 信息增益作为划分准则存在的问题：\n",
    "\n",
    "     信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用**信息增益比**来选择特征，可以校正这个问题。\n",
    "     \n",
    "- 特点\n",
    "  - 能够完成对连续属性的离散化处理\n",
    "  - 能够对不完整数据进行处理\n",
    "  - 需要对数据集进行多次的顺序扫描和排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fc0e69a-a2b4-4727-9e23-03bb4b7d1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen3(data, index, kind, method):\n",
    "    ls = []\n",
    "    if method == 0:\n",
    "        for temp in data:\n",
    "            if temp[index] <= kind:\n",
    "                t = temp[0 : index]\n",
    "                t = t + temp[index + 1 : ]\n",
    "                ls.append(t)\n",
    "    else:\n",
    "        for temp in data:\n",
    "            if temp[index] > kind:\n",
    "                t = temp[0 : index]\n",
    "                t = t + temp[index + 1 : ]\n",
    "                ls.append(t)\n",
    "    return ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用$C4.5$算法构建决策树\n",
    "信息增益作为划分准则存在的问题：<br/>\n",
    "信息增益偏向于选择取值较多的特征进行划分。⽐如学号这个特征，每个学生都有一个不同的学号，如果根据学号对样本进行分类，则每个学生都属于不同的类别，这样是没有意义的。而C4.5在生成过程中，用信息增益比来选择特征，可以校正这个问题。<br/>\n",
    "信息增益比 = 惩罚参数 * 信息增益，即 $g_R(D,A) = \\frac{g(D,A)}{H_A(D)}$，其中的$H_A(D)$，对于样本集合D，将当前特征A作为随机变量（取值是特征A的各个特征值），求得的经验熵。<br/>\n",
    "信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。<br/>\n",
    "惩罚参数：数据集D以特征A作为随机变量的熵的倒数，即：将特征A取值相同的样本划分到同一个子集中，$惩罚参数 = \\frac{1}{H_A(D)}=\\frac{1}{-\\sum_{(i = 1)}^{n}\\frac{|D_i|}{|D|}\\log_2\\frac{|D_i|}{|D|}}$ <br/>\n",
    "- 将连续属性离散化<br/>\n",
    "采用二分法，把数据由小到大排列，选择每个区间的中位点位取值，左侧区间为不大于该点的值，右侧区间为大于该点的值\n",
    "- 将数据按照某一种类的属性或者数值的区间重新分类；method = 0 按照区间左侧分类，method = 1 按照区间右侧分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91a7382-1a12-4dfb-8379-0a6dc07482b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zuijiatezhengfenli2(data):\n",
    "    base, mm= information(data) #原始的信息熵\n",
    "    info = []\n",
    "    for j in range(len(data[0]) - 1):\n",
    "        dic = {}\n",
    "        for i in data:\n",
    "            current = i[j] #取出最后的结果\n",
    "            if current not in dic.keys(): \n",
    "                dic[current] = 1 #创建一个新的类别\n",
    "            else:\n",
    "                dic[current] += 1 #原有类别+1\n",
    "        result = 0.0\n",
    "        for key in dic:\n",
    "            prob = float(dic[key]) / len(data)\n",
    "            result -= prob * math.log(prob,2) \n",
    "        info.append(result)\n",
    "    best = 0\n",
    "    bestindex = -1\n",
    "    bestpartvalue = None #如果是离散值，使用该值进行分割\n",
    "    for i in range(len(data[0]) - 1):\n",
    "        #抽取该列数据的所有信息\n",
    "        ls = [index[i] for index in data]\n",
    "        feture = set(ls) \n",
    "        #计算该列的信息增益\n",
    "        temp = 0.0\n",
    "        if type(ls[0]) == type(\"a\"):#判断是否时离散的\n",
    "            for value in feture:\n",
    "                datatemp = split(data, i, value)\n",
    "                prob = len(datatemp) / float(len(data))\n",
    "                t, mm = information(datatemp)\n",
    "                temp += prob * t\n",
    "        else:\n",
    "            ls.sort()\n",
    "            min = float(\"inf\")\n",
    "            for j in range(len(ls) - 1):\n",
    "                part = (ls[j + 1] + ls[j]) / 2 #计算划分点\n",
    "                left = split2(data, i, part, 0)\n",
    "                right = split2(data, i, part, 1)\n",
    "                temp1, useless = information(left)\n",
    "                temp2, useless = information(right)\n",
    "                temp = len(left) / len(data) * temp1 + len(right) / len(data) * temp2\n",
    "                if temp < min:\n",
    "                    min = temp\n",
    "                    bestpartvalue = part\n",
    "            temp = min\n",
    "        infoGain = base - temp\n",
    "        #根据信息增益挑选 \n",
    "        if info[i] != 0:\n",
    "            if infoGain / info[i] >= best:\n",
    "                best = infoGain / info[i]\n",
    "                bestindex = i\n",
    "    return bestindex, bestpartvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对测试集利用$C4.5$决策树进行分类\n",
    "- 取出当前树的根节点\n",
    "- 利用跟节点信息查询当前输入数据输入内容\n",
    "- 如果是离散值\n",
    "    1. 如果查询出来的分支是叶节点，返回该值\n",
    "    2. 如果不是叶节点，递归查询子树\n",
    "- 如果是非离散值\n",
    "    1. 取出节点值与现在数据进行比较\n",
    "    2. 如果大于以“否”为标签，否则以“是”为标签\n",
    "    3. 如果查询出来的分支是叶节点，返回该值\n",
    "    4. 如果不是叶节点，递归查询子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fafd1ad-19b5-4007-8bdb-76034d0d1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen4(data, labels):\n",
    "    typelist = [index[-1] for index in data] #取出该数据集的分类\n",
    "    nothing, typecount = information(data) #计算出类别种类以及数量\n",
    "    if typecount == len(typelist): #如果只有一个类别\n",
    "        return typelist[0]\n",
    "    bestindex, part = chooseBestFeatureToSplit2(data)  # 最优划分属性的索引\n",
    "    if bestindex == -1:\n",
    "        return \"是\"\n",
    "    if type([t[bestindex] for t in data][0]) == type(\"a\"):#判断是否时离散的\n",
    "        bestlabel = labels[bestindex]\n",
    "        Tree = {bestlabel: {}}\n",
    "        temp = copy.copy(labels)\n",
    "        feature = [example[bestindex] for example in data]\n",
    "        del (temp[bestindex])  # 已经选择的特征不再参与分类，将该类别删除\n",
    "        unique = set(feature)  # 该属性所有可能取值，也就是节点的分支\n",
    "        for i in unique:  \n",
    "            s = temp[:] \n",
    "            Tree[bestlabel][i] = classify2(split(data, bestindex, i), s)\n",
    "    else: #连续的变量\n",
    "        bestlabel = labels[bestindex] + \"<\" + str(part)\n",
    "        Tree = {bestlabel: {}}\n",
    "        temp = labels[:]\n",
    "        del(temp[bestindex])\n",
    "        leftdata = split2(data, bestindex, part, 0)\n",
    "        Tree[bestlabel][\"是\"] = classify2(leftdata, temp)\n",
    "        rightdata = split2(data, bestindex, part, 1)\n",
    "        Tree[bestlabel][\"否\"] = classify2(rightdata, temp)\n",
    "    return Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在构建决策树的过程中，我们进行了预剪枝判断，即计算划分后的准确率，并与原始准确率进行比较，决定是否进行划分。如果划分后的准确率低于原始准确率，则进行剪枝，返回叶子节点。\n",
    "- 剪枝后的决策树 pruned_tree 可以用于进行分类预测，可以使用 run2函数对新的数据进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b1b333d-4f5f-4207-b817-4d5308dbc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fen5(data, labels, validation_data=None, validation_labels=None):\n",
    "    typelist = [index[-1] for index in data]  # 取出该数据集的分类\n",
    "    nothing, typecount = information(data)  # 计算出类别种类以及数量\n",
    "    if typecount == len(typelist):  # 如果只有一个类别\n",
    "        return typelist[0]\n",
    "    \n",
    "    # 判断是否需要进行划分\n",
    "    if validation_data is not None and validation_labels is not None:\n",
    "        original_accuracy = accuracy(validation_data, validation_labels)\n",
    "        bestindex, part = zuijiatezhengfenli2(data)\n",
    "        if bestindex == -1:\n",
    "            return \"是\"\n",
    "        if type([t[bestindex] for t in data][0]) == type(\"a\"):\n",
    "            bestlabel = labels[bestindex]\n",
    "            Tree = {bestlabel: {}}\n",
    "            temp = copy.copy(labels)\n",
    "            feature = [example[bestindex] for example in data]\n",
    "            del(temp[bestindex])\n",
    "            unique = set(feature)\n",
    "            for i in unique:\n",
    "                s = temp[:]\n",
    "                Tree[bestlabel][i] = classify2(split(data, bestindex, i), s, validation_data, validation_labels)\n",
    "        else:\n",
    "            bestlabel = labels[bestindex] + \"<\" + str(part)\n",
    "            Tree = {bestlabel: {}}\n",
    "            temp = labels[:]\n",
    "            del(temp[bestindex])\n",
    "            leftdata = split2(data, bestindex, part, 0)\n",
    "            Tree[bestlabel][\"是\"] = classify2(leftdata, temp, validation_data, validation_labels)\n",
    "            rightdata = split2(data, bestindex, part, 1)\n",
    "            Tree[bestlabel][\"否\"] = classify2(rightdata, temp, validation_data, validation_labels)\n",
    "        \n",
    "        # 计算划分后的准确率，并与原始准确率进行比较\n",
    "        pruned_accuracy = accuracy(validation_data, validation_labels)\n",
    "        if pruned_accuracy >= original_accuracy:\n",
    "            return Tree  # 不剪枝，保持划分\n",
    "        else:\n",
    "            return typelist[0]  # 剪枝，返回叶子节点\n",
    "    else:\n",
    "        bestindex, part = zuijiatezhengfenli2(data)\n",
    "        if bestindex == -1:\n",
    "            return \"是\"\n",
    "        if type([t[bestindex] for t in data][0]) == type(\"a\"):\n",
    "            bestlabel = labels[bestindex]\n",
    "            Tree = {bestlabel: {}}\n",
    "            temp = copy.copy(labels)\n",
    "            feature = [example[bestindex] for example in data]\n",
    "            del(temp[bestindex])\n",
    "            unique = set(feature)\n",
    "            for i in unique:\n",
    "                s = temp[:]\n",
    "                Tree[bestlabel][i] = fen4(split(data, bestindex, i), s)\n",
    "        else:\n",
    "            bestlabel = labels[bestindex] + \"<\" + str(part)\n",
    "            Tree = {bestlabel: {}}\n",
    "            temp = labels[:]\n",
    "            del(temp[bestindex])\n",
    "            leftdata = split2(data, bestindex, part, 0)\n",
    "            Tree[bestlabel][\"是\"] = fen4(leftdata, temp)\n",
    "            rightdata = split2(data, bestindex, part, 1)\n",
    "            Tree[bestlabel][\"否\"] = fen4(rightdata, temp)\n",
    "        return Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算信息熵和信息增益并返回最优的分类方式，对数据的每项指标做以下的过程\n",
    "    1. 抽取该项数据的所有信息\n",
    "    2. 计算每一类的信息熵\n",
    "    3. 按照该项数据的类别信息将数据集划分成多个子数据集\n",
    "    4. 计算每个数据集的信息熵\n",
    "    5. 计算该项数据的信息增益比\n",
    "    6. 根据信息增益选择最好的分类项目，返回该项目的类别号\n",
    "    7. 对连续型数据，计算分割值，求出信息增益比最小的点作为分割点返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5005dea8-c472-4dd4-bbfa-39eb3b3340d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成决策树如下: {'纹理': {'清晰': {'根蒂': {'稍蜷': {'密度<0.3815': {'是': '是', '否': {'色泽': {'青绿': '是', '乌黑': '是'}}}}, '蜷缩': {'密度<0.5820000000000001': {'是': '是', '否': {'敲声': {'浊响': {'色泽': {'青绿': '是', '乌黑': '是'}}, '沉闷': {'色泽': {'青绿': '是', '乌黑': '是'}}}}}}, '硬挺': '是'}}, '模糊': {'密度<0.29400000000000004': {'是': '是', '否': '是'}}, '稍糊': {'敲声': {'浊响': {'密度<0.56': {'是': '是', '否': '是'}}, '沉闷': {'密度<0.6615': {'是': '是', '否': {'根蒂': {'稍蜷': '是', '蜷缩': '是'}}}}}}}}\n",
      "生成决策树如下: {'纹理': {'清晰': {'根蒂': {'稍蜷': {'密度<0.3815': {'是': '是', '否': {'色泽': {'青绿': '是', '乌黑': '是'}}}}, '蜷缩': {'密度<0.5820000000000001': {'是': '是', '否': {'敲声': {'浊响': {'色泽': {'青绿': '是', '乌黑': '是'}}, '沉闷': {'色泽': {'青绿': '是', '乌黑': '是'}}}}}}, '硬挺': '是'}}, '模糊': {'密度<0.29400000000000004': {'是': '是', '否': '是'}}, '稍糊': {'敲声': {'浊响': {'密度<0.56': {'是': '是', '否': '是'}}, '沉闷': {'密度<0.6615': {'是': '是', '否': {'根蒂': {'稍蜷': '是', '蜷缩': '是'}}}}}}}}\n"
     ]
    }
   ],
   "source": [
    "def ce2(data, tree, labels):\n",
    "    firstStr = list(tree.keys())[0]  # 根节点\n",
    "    firstLabel = firstStr\n",
    "    t = str(firstStr).find('<') #查看是否是连续型\n",
    "    if t > -1:  # 如果是连续型的特征\n",
    "        firstLabel = str(firstStr)[ : t]\n",
    "    secondDict = tree[firstStr]\n",
    "    featIndex = labels.index(firstLabel)  # 跟节点对应的属性\n",
    "    result = ''\n",
    "    for key in list(secondDict.keys()):  # 对每个分支循环\n",
    "        if type(data[featIndex]) == type(\"a\"):\n",
    "            if data[featIndex] == key:  # 测试样本进入某个分支\n",
    "                if type(secondDict[key]).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict[key], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict[key]\n",
    "        else:\n",
    "            value = float(str(firstStr)[t + 1 : ])\n",
    "            if data[featIndex] <= value:\n",
    "                if type(secondDict['是']).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict['是'], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict['是']\n",
    "            else:\n",
    "                if type(secondDict['否']).__name__ == 'dict':  # 该分支不是叶子节点，递归\n",
    "                    result = run2(data, secondDict['否'], labels)\n",
    "                else:  # 如果是叶子， 返回结果\n",
    "                    result = secondDict['否']\n",
    "    return result\n",
    "\n",
    "def accuracy(validation_data, validation_labels):\n",
    "    correct = 0\n",
    "    for i in range(len(validation_data)):\n",
    "        if validation_data[i] == validation_labels[i]:\n",
    "            correct += 1\n",
    "    return correct / len(validation_data)\n",
    "def shengc2(train, test, labels):\n",
    "    ls = []   \n",
    "    tree = fen4(train, labels)\n",
    "    # 使用预剪枝方法对决策树进行构建和剪枝\n",
    "    #pruned_tree = classify2(train2, labels2, validation_data=test2, validation_labels=labels2)\n",
    "    print(\"生成决策树如下:\", tree)\n",
    "    for index in test:\n",
    "        ls.append(run2(index, tree, labels))\n",
    "        #ls2.append(run2(index,pruned_tree,labels))\n",
    "    return ls\n",
    "def shengc3(train, test, labels):\n",
    "    ls2 = []\n",
    "    #tree = classify2(train, labels)\n",
    "    # 使用预剪枝方法对决策树进行构建和剪枝\n",
    "    pruned_tree = fen4(train2, labels2, validation_data=test2, validation_labels=labels2)\n",
    "    print(\"生成决策树如下:\", pruned_tree)\n",
    "    for index in test:\n",
    "        #ls.append(run2(index, tree, labels))\n",
    "        ls2.append(run2(index,pruned_tree,labels))\n",
    "    return ls2\n",
    "labels2 = [\"色泽\", \"根蒂\", \"敲声\", \"纹理\", \"密度\", \"好瓜\"]\n",
    "result2 = shengc2(train2, test2, labels2)\n",
    "result3 = shengc3(train2, test2, labels2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 递归构建$C4.5$决策树\n",
    "- 决策树的生成\n",
    "\t1. 从根节点开始，计算所有可能特征的信息增益，选择信息增益最大的特征作为划分该节点的特征，根据该特征的不同取值建立子节点；\n",
    "\t2. 在对子节点递归地调用以上方法，直到达到停止条件，得到⼀个决策树。\n",
    "- 迭代停止条件\n",
    "    1. 当前结点所有样本都属于同⼀类别；\n",
    "    2. 当前结点的所有属性值都相同，没有剩余属性可用来进一步划分样本；\n",
    "    3. 达到最大树深；\n",
    "    4. 达到叶子结点的最小样本数；\n",
    "- 具体实现\n",
    "    1. 首先取出该数据集的类别信息\n",
    "    2. 统计处类别信息以及数量，如果只有一个类别，返回该类别\n",
    "    3. 计算最优划分的索引\n",
    "    4. 初始化子树\n",
    "    5. 当前已经选择的特征如果是离散的便不再参与分类，将该特征删除；如果是连续的不删除该特征，以分界点构建左子树和右子树\n",
    "    6. 计算剩余特征的集合\n",
    "    7. 对于每个分支，进行递归\n",
    "    8. 返回值为子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6efd7362-ddec-4127-9d4a-482af28a041f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C4.5分类器对于test2数据集的准确率是： 60.00%\n",
      "C4.5剪枝分类器对于test2数据集的准确率是： 60.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"C4.5分类器对于test2数据集的准确率是：\", rate(test2, result2))\n",
    "print(\"C4.5剪枝分类器对于test2数据集的准确率是：\", rate(test2, result3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算准确率\n",
    "可以看出，C4.5预测的准确率稍较为一般"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7b460-207d-438f-a3ee-a004255d38a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结\n",
    "本次实验进行了决策树的判断以及决策树构造剪枝等等的编程工作，我们在使用过常见的调库解决决策树问题之后采用现在的原始方法构造解决决策树问题，能够更加深刻的领悟到决策树算法的原理，对决策树的应用也能更加的得心应手，面对决策树问题的时候也能够更加的熟练。同时为大作业的编程提供了一定的参考意义。\n",
    "本次实验当中采用了剪枝算法，我们构造的决策树经历了剪枝之后变化不大，原因可能是构造的决策树较小，样本集仍然不够大，需要我们不断的扩展决策树，才能够更好的展现决策树的实现价值。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
